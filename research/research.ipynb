{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6735f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90058ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pytesseract\n",
    "import cv2\n",
    "import streamlit as st\n",
    "import os\n",
    "import tempfile\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import datefinder\n",
    "from datetime import datetime\n",
    "from keybert import KeyBERT\n",
    "from PyPDF2 import PdfReader\n",
    "import docx2txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94236fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf129828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should print: True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee71dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"Device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f4eb26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Harshal\n",
      "[nltk_data]     Sharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Harshal\n",
      "[nltk_data]     Sharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Harshal\n",
      "[nltk_data]     Sharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Harshal\n",
      "[nltk_data]     Sharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Harshal\n",
      "[nltk_data]     Sharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#python -m nltk.downloader punkt stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11621ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 25.1 from C:\\Anaconda\\envs\\meta-gen\\lib\\site-packages\\pip (python 3.10)\n",
      "Requirement already satisfied: pdfminer.six in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (20250506)\n",
      "Requirement already satisfied: python-docx in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: pytesseract in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: opencv-python in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: keybert in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: streamlit in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (1.46.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from pdfminer.six) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from pdfminer.six) (45.0.4)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from python-docx) (5.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from pytesseract) (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from opencv-python) (2.1.2)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from keybert) (14.0.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from keybert) (1.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from sentence-transformers) (2.7.1+cu118)\n",
      "Requirement already satisfied: scipy in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from streamlit) (6.1.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from streamlit) (8.2.1)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from streamlit) (2.3.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from streamlit) (6.31.1)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from streamlit) (20.0.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.43.1)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.6.15)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from rich>=10.4.0->keybert) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda\\envs\\meta-gen\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six python-docx pytesseract opencv-python keybert sentence-transformers streamlit --progress-bar on --no-cache-dir -v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a33efe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4487cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "06cc2c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19f00e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Harshal Sharma\\\\Desktop\\\\meta-gen\\\\research'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52a02d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harshal Sharma\\Desktop\\meta-gen\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "471e18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "638f255a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE-INTELLIGENT-INVESTOR.pdf', 'uploads']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62fbfff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(base_dir,files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b9001da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\uploads'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ece47777",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "38ab3fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MARS OPEN PROJECTS 2025 (1).pdf']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a54b876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(path,file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8ebc51c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\uploads\\\\MARS OPEN PROJECTS 2025 (1).pdf'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cbc13b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text(path):\n",
    "    file_ext = Path(path).suffix.lower()\n",
    "\n",
    "    if file_ext == '.pdf':\n",
    "        reader = PdfReader(path)\n",
    "        return \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "\n",
    "    elif file_ext == '.docx':\n",
    "        return docx2txt.process(path)\n",
    "\n",
    "    elif file_ext == '.txt':\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    elif file_ext in ['.jpg', '.jpeg', '.png']:\n",
    "        img = cv2.imread(path)\n",
    "        return pytesseract.image_to_string(img)\n",
    "\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d084d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_lemmatize_nltk(text):\n",
    "    # Remove special characters and normalize spaces\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "\n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return \" \".join(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "25be3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text(path)\n",
    "cleaned_text = clean_and_lemmatize_nltk(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "026d072c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'open project 2025 aiml problem statement 1 design implement robust pipeline perform emotion classification speech data objective objective project design implement end toend pipeline emotion classification using speech data system leverage audio processing technique machine learningdeep learning model accurately identify categorize emotional state conveyed speechsong dataset dataset description file available n given link note download file filename downloaded informed doubt group audio dataset evaluation criterion model accuracy j udged validation data condition confusion matrix judging criterion f1 score greater 80 accuracy class greater 75 overall accuracy greater 80 apart validation data checking accuracy custom test dataset shared publicly deliverable 1 github repository containing ipynb notebook full running code b trained model c python script file model tested feeding test data inclu de clear detailed readmemd including project description pre processing methodology model pipeline accuracy metric e demo video around 2 minute showing use case web app 2 fully functional web app hosted using stream lit receive audio file input return classified emotion group link 2 automated meta data generation objective project aim develop automated metadata generation system system enhance document discoverability classification subsequent analysis producing scalable consistent semantically rich metadata key area focus development system involve automating metadata generation system must automatically generate metadata various unstructured document ypes content extraction capability extracting text content diverse format like pdf docx txt incorporating optical character recognition ocr necessary essential semantic content identification system need identify leverage meaningful section document inform metadata generation structured metadata creation system responsible generating structured metadata output user interface development intuitive web interface docu ment upload metadata viewing core component system deployment entire system deployed ensure public accessibility ease use deliverable github repository containing ipynb notebook full running code include clea r detailed readmemd file demo video around 2 minute showing use case web app group link web development problem statement 1 collaborative whiteboard real time draw ing objective design develop realtime collaborative whiteboard application allows multiple user draw write interact simultaneously replicating experience physical whiteboard web key feature drawing tool pen shape rectangle circle text tool eraser color picker real time sync real time update using websocket user see change instantly multi user collaboration user join session via shared link collaborate live access control crea te public private room permission editview save export option save whiteboard image pdf canvas management undoredo action clear canvas button optional voice chat real time commentchat sidebar tech stack suggestion frontend reactjs nextjs html5 canvas fabricjs backend nodejs socketio database mongodb firebase roomusersession data hosting vercelnetlify frontend renderheroku backend submission instruction 1 github repository push complete project code public github repo b include clear detailed readmemd project description ii feature iii tech stack used iv setup instruction run project locally v deployed demo link 2 demo video upload screen record ed video 5 10 min demonstrating core feature working live b upload google dri youtube share link group link 2 data compression decompression portal object ive design develop web application allows user upload file apply various data compression algorithm huffman coding run length encoding lz77 reduce file size well decompress previously compressed file system demonstrate efficiency different algorithm providing compression ratio allow user download processed file key feature file upload user upload file text image binary multiple compression algorithm support popular algorithm like huffman coding run length encoding rle lz77 least two compression decompression user compress decompress file using chosen algorithm compression statistic display compression ratio origin al size compressed size processing time download processed file allow user download compressed decompressed file algorithm explanation provide brief description chosen compression algorithm educate user error handling proper feedback unsupported file decompression error responsive ui intuitive frontend react easy file management status update tech stack suggestion frontend framework reactjs responsive dynamic ui styling tailwin cs material ui bootstrap responsive design ui component file handling javascript filereader api third party library read display file metadata visualization optional brownie point chartjs d3js compression statistic visualization backend runtime nodejs efficient file handling performance framework expressjs creating restful apis compression algorithm implement huffman coding rle lz77 custom javascript module webassembly better performance alternatively use python flask fastapi leveraging existing compression library advanced algorithm file handling multer file uploads f module file processing database optional want sto user sessionshistory mongodb nosql storing user info uploaded file metadata compression log firebase want quick scalable serverless option hosting frontend vercel netlify easy integration react apps backend render roku railway deploying nodejs python backend submission instruction 1 github repository push complete project code public github repository b include clear detailed readmemd project description ii feature iii tech stack used iv setup instruction run project locally v deployed demo link 2 demo video upload screen recorded video 5 10 min demonstrating core feature working live b upload google drive youtube share link group link plagiarism notice submission found copied github team disqualified work must original done youyour team feel free use open source toolslibraries ensure implementation logic'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "adae6985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(text):\n",
    "    # Split by lines\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "\n",
    "    # Clean lines and remove empty ones\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "    if not lines:\n",
    "        return \"Unknown Title\"\n",
    "\n",
    "    # Heuristic: longest first line with reasonable length (5–100 chars)\n",
    "    for line in lines:\n",
    "        if 5 < len(line) < 100 and line[0].isupper():\n",
    "            return line\n",
    "\n",
    "    return lines[0]  # fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "235ee76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_author(text):\n",
    "    # Step 1: First try to detect lines like \"By Benjamin Graham\"\n",
    "    lines = text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        match = re.search(r\"\\b(By|Written by|Author:)\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\", line)\n",
    "        if match:\n",
    "            return match.group(2)\n",
    "\n",
    "    # Step 2: Fallback to NER (find first PERSON entity)\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return ent.text\n",
    "\n",
    "    return \"Unknown Author\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6cf9b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(text):\n",
    "    matches = list(datefinder.find_dates(text))\n",
    "    \n",
    "    # Optional: filter future dates if not useful\n",
    "    matches = [d for d in matches if d.year <= datetime.now().year]\n",
    "\n",
    "    if matches:\n",
    "        # Return the first detected date (assumed as publishing/mention)\n",
    "        return matches[0].strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    return \"Unknown Date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d8d2f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')  # Fast & good quality\n",
    "\n",
    "def extract_keywords(text, top_n=5):\n",
    "    keywords = kw_model.extract_keywords(text, top_n=top_n, stop_words='english')\n",
    "    # keywords is a list of tuples: (keyword, score)\n",
    "    return [kw[0] for kw in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dbe7dadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a7ed312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, max_chunk_size=3000):\n",
    "    # Clean up whitespace\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "\n",
    "    chunks = []\n",
    "    while len(text) > max_chunk_size:\n",
    "        # Find nearest period before max_chunk_size\n",
    "        split_index = text.rfind('.', 0, max_chunk_size)\n",
    "        if split_index == -1:\n",
    "            split_index = max_chunk_size  # fallback\n",
    "        chunks.append(text[:split_index + 1].strip())\n",
    "        text = text[split_index + 1:].strip()\n",
    "\n",
    "    if text:\n",
    "        chunks.append(text)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1df39992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text, min_len=40, max_len=150):\n",
    "    chunks = split_text_into_chunks(text)\n",
    "    print(f\"Total Chunks: {len(chunks)}\")\n",
    "\n",
    "    final_summary = \"\"\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        summary = summarizer(chunk, max_length=max_len, min_length=min_len, do_sample=False)[0]['summary_text']\n",
    "        final_summary += summary.strip() + \" \"\n",
    "\n",
    "    return final_summary.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3f1e44e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 18. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Summary:\n",
      "\n",
      "open project 2025 aiml problem statement 1 design implement robust pipeline perform emotion classification speech data objective objective project design implement end toend pipeline emotion classification using speech data system. 2 fully functional web app hosted using stream lit receive audio file input return classified emotion group link 2 automated meta data generation objective project aim develop automated metadata generation system system enhance document discoverability classification subsequent analysis. Project is a web application that allows users to upload files. The project includes a demo video demonstrating core feature working live. The video was uploaded to YouTube and is part of a larger project. The code is available on GitHub. your team feel free to use open source tools. libraries ensure implementation logic. al done you. Use these tools to help your team get the most out of open source software. Use this tool to help you get the best out of your software.\n"
     ]
    }
   ],
   "source": [
    "summary_output = generate_summary(cleaned_text)\n",
    "print(\"Final Summary:\\n\")\n",
    "print(summary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "de019a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)  # use -1 for CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6306c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_category_from_chunks(text, candidate_labels=None, max_chunk_size=1000):\n",
    "    from collections import Counter\n",
    "\n",
    "    if candidate_labels is None:\n",
    "        candidate_labels = [\"Finance\", \"Health\", \"Education\", \"Politics\", \"Technology\", \"History\", \"Philosophy\", \"Biography\", \"Science\", \"Fiction\"]\n",
    "\n",
    "    # Chunk the text manually\n",
    "    chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
    "    \n",
    "    scores = Counter()\n",
    "\n",
    "    for chunk in chunks:\n",
    "        result = classifier(chunk, candidate_labels)\n",
    "        top_label = result['labels'][0]\n",
    "        confidence = result['scores'][0]\n",
    "        scores[top_label] += confidence\n",
    "\n",
    "    # Pick the category with highest total confidence\n",
    "    best_category = scores.most_common(1)[0][0]\n",
    "    return best_category, dict(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text(path)                   # Raw full text from uploaded file\n",
    "title = extract_title(text)                # On raw\n",
    "author = extract_author(text)              # On raw\n",
    "date = extract_date(text)                  # On raw\n",
    "summary = generate_summary(text)           # On raw\n",
    "category, scores = detect_category_from_chunks(text)  # On raw\n",
    "keywords = extract_keywords(text)          # Can use raw or cleaned\n",
    "\n",
    "# Optional\n",
    "cleaned_text = clean_and_lemmatize_nltk(text)  # Only for extra features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c4f9e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metadata(text):\n",
    "    metadata = {}\n",
    "\n",
    "    # Step-by-step extraction\n",
    "    metadata[\"title\"] = extract_title(text)\n",
    "    metadata[\"author\"] = extract_author(text)\n",
    "    metadata[\"date\"] = extract_date(text)\n",
    "    metadata[\"summary\"] = generate_summary(text)\n",
    "    metadata[\"category\"], metadata[\"category_scores\"] = detect_category_from_chunks(text)\n",
    "    metadata[\"keywords\"] = extract_keywords(text)\n",
    "\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a5882a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 3\n",
      "{'author': 'Deliverables',\n",
      " 'category': 'Technology',\n",
      " 'category_scores': {'Health': 0.28131720423698425,\n",
      "                     'Technology': 3.31836499273777},\n",
      " 'date': '2025-06-22',\n",
      " 'keywords': ['voice', 'classification', 'speech', 'audio', 'emotion'],\n",
      " 'summary': 'The project aims to design and implement an end -to-end pipeline '\n",
      "            'for emotion classification using speech data. The system will '\n",
      "            'leverage audio processing techniques and machine learning/deep '\n",
      "            'learning models to accurately identify and categorize emotional '\n",
      "            'states conveyed in speech/song. The project also aims to develop '\n",
      "            'an automated metadata generation system. Design and develop a '\n",
      "            'real-time collaborative whiteboard application that allows  '\n",
      "            'multiple users to draw, write, and interact simultaneously, '\n",
      "            'replicating the  experience of a physical whiteboard on the web. '\n",
      "            'Frontend: React.js or Next.js,  HTML5  Canvas or Fabric.js. '\n",
      "            'Backend:  Node.js + Socket.io. Demo video of around 2 -minutes '\n",
      "            'showing the use -case of web -app. Frontend: React.js (for a '\n",
      "            'responsive and dynamic UI) Backend: Render / He roku / Railway '\n",
      "            '(for deploying Node.js or Python backend) Demo: Upload a screen '\n",
      "            '-recorded video (5 –10 mins) demonstrating all the core features '\n",
      "            'working live.',\n",
      " 'title': 'OPEN PROJECTS ( 2025 )'}\n"
     ]
    }
   ],
   "source": [
    "raw_text = extract_text(path)\n",
    "metadata = generate_metadata(raw_text)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1adeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
